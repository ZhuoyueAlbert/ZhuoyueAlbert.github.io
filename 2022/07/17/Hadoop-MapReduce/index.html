

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Zhao Zhuoyue">
  <meta name="keywords" content="">
  
    <meta name="description" content="MapReduceMapReduce概述MapReduce 定义MapReduce 分布式运算程序编程框架，将用户编写的业务逻辑代码和自带的默认组件整合成一个完整的分布式运算程序，并发运行在一个 hadoop 集群上。 MapReduce 优缺点优点 MapReduce 易于编程：它简单的实现一些接口，就可以完成一个分布式程序 良好的扩展性：通过简单地增加机器数量来提高它的计算能力 高容错性：自动">
<meta property="og:type" content="article">
<meta property="og:title" content="MapReduce">
<meta property="og:url" content="http://example.com/2022/07/17/Hadoop-MapReduce/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="MapReduceMapReduce概述MapReduce 定义MapReduce 分布式运算程序编程框架，将用户编写的业务逻辑代码和自带的默认组件整合成一个完整的分布式运算程序，并发运行在一个 hadoop 集群上。 MapReduce 优缺点优点 MapReduce 易于编程：它简单的实现一些接口，就可以完成一个分布式程序 良好的扩展性：通过简单地增加机器数量来提高它的计算能力 高容错性：自动">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2022/07/17/Hadoop-MapReduce/image-20220715112714270.png">
<meta property="og:image" content="http://example.com/2022/07/17/Hadoop-MapReduce/MR_Procedure.jpg">
<meta property="og:image" content="http://example.com/2022/07/17/Hadoop-MapReduce/image-20220716154211817.png">
<meta property="og:image" content="http://example.com/2022/07/17/Hadoop-MapReduce/image-20220716154914368.png">
<meta property="article:published_time" content="2022-07-17T08:32:00.000Z">
<meta property="article:modified_time" content="2023-02-12T08:23:02.000Z">
<meta property="article:author" content="Zhao Zhuoyue">
<meta property="article:tag" content="Hadoop">
<meta property="article:tag" content="MapReduce">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://example.com/2022/07/17/Hadoop-MapReduce/image-20220715112714270.png">
  
  
  
  <title>MapReduce - Hexo</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.2","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 6.2.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Fluid</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/links/">
                <i class="iconfont icon-link-fill"></i>
                友链
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="MapReduce"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2022-07-17 16:32" pubdate>
          2022年7月17日 下午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          22k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          182 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">MapReduce</h1>
            
            
              <div class="markdown-body">
                
                <h1 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h1><h2 id="MapReduce概述"><a href="#MapReduce概述" class="headerlink" title="MapReduce概述"></a>MapReduce概述</h2><h3 id="MapReduce-定义"><a href="#MapReduce-定义" class="headerlink" title="MapReduce 定义"></a>MapReduce 定义</h3><p>MapReduce <code>分布式运算程序编程</code>框架，将用户编写的<code>业务逻辑代码</code>和<code>自带的默认组件</code>整合成一个完整的分布式运算程序，并发运行在一个 hadoop 集群上。</p>
<h3 id="MapReduce-优缺点"><a href="#MapReduce-优缺点" class="headerlink" title="MapReduce 优缺点"></a>MapReduce 优缺点</h3><h4 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h4><ol>
<li>MapReduce 易于编程：它简单的实现一些接口，就可以完成一个分布式程序</li>
<li>良好的扩展性：通过简单地增加机器数量来提高它的计算能力</li>
<li>高容错性：自动处理某些问题，如其中一台机器挂了，它可以把上面的计算任务转移到另一个节点上运行，不至于这个任务运行</li>
<li>适合 PB 级以上海量数据的离线处理</li>
</ol>
<h4 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h4><ol>
<li>不擅长实时计算</li>
<li>不擅长流式计算</li>
<li>不擅长 DAG （有向无环图）计算</li>
</ol>
<h3 id="MapReduce-进程"><a href="#MapReduce-进程" class="headerlink" title="MapReduce 进程"></a>MapReduce 进程</h3><blockquote>
<ol>
<li>MrAppMaster： 负责整个程序的过程调度及状态调度</li>
<li>MapTask: 负责 Map 阶段的整个数据处理流程</li>
<li>ReduceTask: 负责 Reduce 阶段的整个数据处理流程</li>
</ol>
</blockquote>
<h3 id="Word-Count"><a href="#Word-Count" class="headerlink" title="Word Count"></a>Word Count</h3><ol>
<li><p>按照 MapReduce 编程规范，分别编写 Mapper，Reducer， Driver。</p>
</li>
<li><p>准备环境</p>
<p>在 pom.xml 文件中添加如下依赖</p>
</li>
</ol>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">dependencies</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">dependency</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span>org.apache.hadoop<span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span>hadoop-client<span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span>3.1.3<span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">dependency</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">dependency</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span>junit<span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span>junit<span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span>4.12<span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">dependency</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">dependency</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span>org.slf4j<span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span>slf4j-log4j12<span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span>1.7.30<span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">dependency</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">dependencies</span>&gt;</span><br></code></pre></td></tr></table></figure>

<p>​	在项目的 src&#x2F;main&#x2F;resources 目录下，新建一个文件，命名为“log4j.properties”，在<br>文件中填入。  </p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-attr">log4j.rootLogger</span>=INFO, stdout<br><span class="hljs-attr">log4j.appender.stdout</span>=org.apache.log4j.ConsoleAppender<br><span class="hljs-attr">log4j.appender.stdout.layout</span>=org.apache.log4j.PatternLayout<br><span class="hljs-attr">log4j.appender.stdout.layout.ConversionPattern</span>=%d %p [%c] - %m%n<br><span class="hljs-attr">log4j.appender.logfile</span>=org.apache.log4j.FileAppender<br><span class="hljs-attr">log4j.appender.logfile.File</span>=target/spring.log<br><span class="hljs-attr">log4j.appender.logfile.layout</span>=org.apache.log4j.PatternLayout<br><span class="hljs-attr">log4j.appender.logfile.layout.ConversionPattern</span>=%d %p [%c] - %m%n<br></code></pre></td></tr></table></figure>

<ol start="3">
<li><p>编写程序</p>
<p>编写 Mapper 类</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.test.mapreduce.wordcount;<br><span class="hljs-keyword">import</span> java.io.IOException;<br><span class="hljs-keyword">import</span> org.apache.hadoop.io.IntWritable;<br><span class="hljs-keyword">import</span> org.apache.hadoop.io.LongWritable;<br><span class="hljs-keyword">import</span> org.apache.hadoop.io.Text;<br><span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.Mapper;<br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">WordCountMapper</span> <span class="hljs-keyword">extends</span> <span class="hljs-title class_">Mapper</span>&lt;LongWritable, Text, Text, IntWritable&gt;&#123;<br>    <span class="hljs-type">Text</span> <span class="hljs-variable">k</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Text</span>();<br>    <span class="hljs-type">IntWritable</span> <span class="hljs-variable">v</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">IntWritable</span>(<span class="hljs-number">1</span>);<br>    <span class="hljs-meta">@Override</span><br>    <span class="hljs-keyword">protected</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">map</span><span class="hljs-params">(LongWritable key, Text value, Context context)</span><br>    <span class="hljs-keyword">throws</span> IOException, InterruptedException &#123;<br>        <span class="hljs-comment">// 1 获取一行</span><br>        <span class="hljs-type">String</span> <span class="hljs-variable">line</span> <span class="hljs-operator">=</span> value.toString();<br>        <span class="hljs-comment">// 2 切割</span><br>        String[] words = line.split(<span class="hljs-string">&quot; &quot;</span>);<br>        <span class="hljs-comment">// 3 输出</span><br>        <span class="hljs-keyword">for</span> (String word : words) &#123;<br>            k.set(word);<br>            context.write(k, v);<br>        &#125;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure>

<p>​	编写 Reducer 类  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.test.mapreduce.wordcount;<br><span class="hljs-keyword">import</span> java.io.IOException;<br><span class="hljs-keyword">import</span> org.apache.hadoop.io.IntWritable;<br><span class="hljs-keyword">import</span> org.apache.hadoop.io.Text;<br><span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.Reducer;<br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">WordCountReducer</span> <span class="hljs-keyword">extends</span> <span class="hljs-title class_">Reducer</span>&lt;Text, IntWritable, Text, IntWritable&gt;&#123;<br>    <span class="hljs-type">int</span> sum;<br>    <span class="hljs-type">IntWritable</span> <span class="hljs-variable">v</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">IntWritable</span>();<br>    <span class="hljs-meta">@Override</span><br>    <span class="hljs-keyword">protected</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">reduce</span><span class="hljs-params">(Text key, Iterable&lt;IntWritable&gt; values,Context</span><br><span class="hljs-params">    context)</span> <span class="hljs-keyword">throws</span> IOException, InterruptedException &#123;<br>        <span class="hljs-comment">// 1 累加求和</span><br>        sum = <span class="hljs-number">0</span>;<br>        <span class="hljs-keyword">for</span> (IntWritable count : values) &#123;<br>            sum += count.get();<br>        &#125;<br>        <span class="hljs-comment">// 2 输出</span><br>        v.set(sum);<br>        context.write(key,v);<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure>

<p>​	编写 Driver 驱动类</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.test.mapreduce.wordcount;<br><span class="hljs-keyword">import</span> java.io.IOException;<br><span class="hljs-keyword">import</span> org.apache.hadoop.conf.Configuration;<br><span class="hljs-keyword">import</span> org.apache.hadoop.fs.Path;<br><span class="hljs-keyword">import</span> org.apache.hadoop.io.IntWritable;<br><span class="hljs-keyword">import</span> org.apache.hadoop.io.Text;<br><span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.Job;<br><span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;<br><span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;<br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">WordCountDriver</span> &#123;<br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">main</span><span class="hljs-params">(String[] args)</span> <span class="hljs-keyword">throws</span> IOException, ClassNotFoundException, InterruptedException &#123;<br>    <span class="hljs-comment">// 1 获取配置信息以及获取 job 对象</span><br>    <span class="hljs-type">Configuration</span> <span class="hljs-variable">conf</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Configuration</span>();<br>    <span class="hljs-type">Job</span> <span class="hljs-variable">job</span> <span class="hljs-operator">=</span> Job.getInstance(conf);<br>    <span class="hljs-comment">// 2 关联本 Driver 程序的 jar</span><br>    job.setJarByClass(WordCountDriver.class);<br>    <span class="hljs-comment">// 3 关联 Mapper 和 Reducer 的 jar</span><br>    job.setMapperClass(WordCountMapper.class);<br>    job.setReducerClass(WordCountReducer.class);<br>    <span class="hljs-comment">// 4 设置 Mapper 输出的 kv 类型</span><br>    job.setMapOutputKeyClass(Text.class);<br>    job.setMapOutputValueClass(IntWritable.class);<br>    <span class="hljs-comment">// 5 设置最终输出 kv 类型</span><br>    job.setOutputKeyClass(Text.class);<br>    job.setOutputValueClass(IntWritable.class);<br>    <span class="hljs-comment">// 6 设置输入和输出路径</span><br>    FileInputFormat.setInputPaths(job, <span class="hljs-keyword">new</span> <span class="hljs-title class_">Path</span>(args[<span class="hljs-number">0</span>]));<br>    FileOutputFormat.setOutputPath(job, <span class="hljs-keyword">new</span> <span class="hljs-title class_">Path</span>(args[<span class="hljs-number">1</span>]));<br>    <span class="hljs-comment">// 7 提交 job</span><br>    <span class="hljs-type">boolean</span> <span class="hljs-variable">result</span> <span class="hljs-operator">=</span> job.waitForCompletion(<span class="hljs-literal">true</span>);<br>    System.exit(result ? <span class="hljs-number">0</span> : <span class="hljs-number">1</span>);<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure></li>
</ol>
<h2 id="Hadoop-序列化"><a href="#Hadoop-序列化" class="headerlink" title="Hadoop 序列化"></a>Hadoop 序列化</h2><ol>
<li>什么是序列化</li>
</ol>
<blockquote>
<p>序列化：把内存中的对象，转换成字节序列（或其他数据传输协议）以便于存储到磁盘（持久化）和网络传输。</p>
<p>反序列化：将收到的字节序列（或其他数据传输协议）或者是磁盘的持久化数据，转为内存中的对象。</p>
</blockquote>
<ol start="2">
<li><p>内存中的对象只能在本地进程中使用而且关机就没了，序列化可以存储对象，在需要的时候调用，也可以将其发送到远程计算机使用。</p>
</li>
<li><p>Java 的序列化是一个重量级序列化框架（Serializable），一个对象被序列化后，会附带很多额外的信息（各种校验信息， Header，继承体系等），不便于在网络中高效传输。所以，Hadoop 自己开发了一套序列化机制（Writable）。</p>
</li>
<li><p>Hadoop 序列化特点：</p>
</li>
</ol>
<blockquote>
<p>紧凑 ： 高效使用存储空间。</p>
<p>快速： 读写数据的额外开销小。</p>
<p>互操作： 支持多语言的交互  。</p>
</blockquote>
<h4 id="自定义-bean-对象实现序列化接口（Writable）"><a href="#自定义-bean-对象实现序列化接口（Writable）" class="headerlink" title="自定义 bean 对象实现序列化接口（Writable）"></a>自定义 bean 对象实现序列化接口（Writable）</h4><ol>
<li>必须实现 Writable 接口  </li>
<li>反序列化时，需要反射调用空参构造函数，所以必须有空参构造  </li>
<li>重写序列化方法  </li>
<li>重写反序列化方法  </li>
<li>注意反序列化的顺序和序列化的顺序完全一致  </li>
<li>要想把结果显示在文件中，需要重写 toString()，可用”\t”分开，方便后续用。  </li>
<li>如果需要将自定义的 bean 放在 key 中传输，则还需要实现 Comparable 接口，因为MapReduce 框中的 Shuffle 过程要求对 key 必须能排序。</li>
</ol>
<p>例如：</p>
<p>统计每一个手机号耗费的总上行流量、 总下行流量、总流量  </p>
<p>输入数据格式：</p>
<table>
<thead>
<tr>
<th>id</th>
<th>手机号码</th>
<th>网络ip</th>
<th>上行流量</th>
<th>下行流量</th>
<th>网络状态码</th>
</tr>
</thead>
<tbody><tr>
<td>7</td>
<td>13560436666</td>
<td>120.196.100.99</td>
<td>1116</td>
<td>954</td>
<td>200</td>
</tr>
</tbody></table>
<p>期望输出数据格式：</p>
<table>
<thead>
<tr>
<th>手机号码</th>
<th>上行流量</th>
<th>下行流量</th>
<th>总流量</th>
</tr>
</thead>
<tbody><tr>
<td>13560436666</td>
<td>1116</td>
<td>954</td>
<td>2070</td>
</tr>
</tbody></table>
<p>编写流量统计的 Bean 对象  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.test.mapreduce.writable;<br><span class="hljs-keyword">import</span> org.apache.hadoop.io.Writable;<br><span class="hljs-keyword">import</span> java.io.DataInput;<br><span class="hljs-keyword">import</span> java.io.DataOutput;<br><span class="hljs-keyword">import</span> java.io.IOException;<br><span class="hljs-comment">//1 继承 Writable 接口</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">FlowBean</span> <span class="hljs-keyword">implements</span> <span class="hljs-title class_">Writable</span> &#123;<br>    <span class="hljs-keyword">private</span> <span class="hljs-type">long</span> upFlow; <span class="hljs-comment">//上行流量</span><br>    <span class="hljs-keyword">private</span> <span class="hljs-type">long</span> downFlow; <span class="hljs-comment">//下行流量</span><br>    <span class="hljs-keyword">private</span> <span class="hljs-type">long</span> sumFlow; <span class="hljs-comment">//总流量</span><br>    <span class="hljs-comment">//2 提供无参构造</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-title function_">FlowBean</span><span class="hljs-params">()</span> &#123;<br>    &#125;<br>    <span class="hljs-comment">//3 提供三个参数的 getter 和 setter 方法</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-type">long</span> <span class="hljs-title function_">getUpFlow</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-keyword">return</span> upFlow;<br>    &#125;<br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">setUpFlow</span><span class="hljs-params">(<span class="hljs-type">long</span> upFlow)</span> &#123;<br>        <span class="hljs-built_in">this</span>.upFlow = upFlow;<br>    &#125;<br>    <span class="hljs-keyword">public</span> <span class="hljs-type">long</span> <span class="hljs-title function_">getDownFlow</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-keyword">return</span> downFlow;<br>    &#125;<br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">setDownFlow</span><span class="hljs-params">(<span class="hljs-type">long</span> downFlow)</span> &#123;<br>        <span class="hljs-built_in">this</span>.downFlow = downFlow;<br>    &#125;<br>    <span class="hljs-keyword">public</span> <span class="hljs-type">long</span> <span class="hljs-title function_">getSumFlow</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-keyword">return</span> sumFlow;<br>    &#125;<br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">setSumFlow</span><span class="hljs-params">(<span class="hljs-type">long</span> sumFlow)</span> &#123;<br>        <span class="hljs-built_in">this</span>.sumFlow = sumFlow;<br>    &#125;<br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">setSumFlow</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-built_in">this</span>.sumFlow = <span class="hljs-built_in">this</span>.upFlow + <span class="hljs-built_in">this</span>.downFlow;<br>    &#125;<br>    <span class="hljs-comment">//4 实现序列化和反序列化方法,注意顺序一定要保持一致</span><br>    <span class="hljs-meta">@Override</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">write</span><span class="hljs-params">(DataOutput dataOutput)</span> <span class="hljs-keyword">throws</span> IOException &#123;<br>        dataOutput.writeLong(upFlow);<br>        dataOutput.writeLong(downFlow);<br>        dataOutput.writeLong(sumFlow);<br>    &#125;<br>    <span class="hljs-meta">@Override</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">readFields</span><span class="hljs-params">(DataInput dataInput)</span> <span class="hljs-keyword">throws</span> IOException &#123;<br>        <span class="hljs-built_in">this</span>.upFlow = dataInput.readLong();<br>        <span class="hljs-built_in">this</span>.downFlow = dataInput.readLong();<br>        <span class="hljs-built_in">this</span>.sumFlow = dataInput.readLong();<br>    &#125;<br>    <span class="hljs-comment">//5 重写 ToString</span><br>    <span class="hljs-meta">@Override</span><br>    <span class="hljs-keyword">public</span> String <span class="hljs-title function_">toString</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-keyword">return</span> upFlow + <span class="hljs-string">&quot;\t&quot;</span> + downFlow + <span class="hljs-string">&quot;\t&quot;</span> + sumFlow;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure>

<p>​	编写 Mapper 类  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.test.mapreduce.writable;<br><span class="hljs-keyword">import</span> org.apache.hadoop.io.LongWritable;<br><span class="hljs-keyword">import</span> org.apache.hadoop.io.Text;<br><span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.Mapper;<br><span class="hljs-keyword">import</span> java.io.IOException;<br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">FlowMapper</span> <span class="hljs-keyword">extends</span> <span class="hljs-title class_">Mapper</span>&lt;LongWritable, Text, Text, FlowBean&gt;<br>&#123;<br>    <span class="hljs-keyword">private</span> <span class="hljs-type">Text</span> <span class="hljs-variable">outK</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Text</span>();<br>    <span class="hljs-keyword">private</span> <span class="hljs-type">FlowBean</span> <span class="hljs-variable">outV</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">FlowBean</span>();<br>    <span class="hljs-meta">@Override</span><br>    <span class="hljs-keyword">protected</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">map</span><span class="hljs-params">(LongWritable key, Text value, Context context)</span><br>    <span class="hljs-keyword">throws</span> IOException, InterruptedException &#123;<br>        <span class="hljs-comment">//1 获取一行数据,转成字符串</span><br>        <span class="hljs-type">String</span> <span class="hljs-variable">line</span> <span class="hljs-operator">=</span> value.toString();<br>        <span class="hljs-comment">//2 切割数据</span><br>        String[] split = line.split(<span class="hljs-string">&quot;\t&quot;</span>);<br>        <span class="hljs-comment">//3 抓取我们需要的数据:手机号,上行流量,下行流量</span><br>        <span class="hljs-type">String</span> <span class="hljs-variable">phone</span> <span class="hljs-operator">=</span> split[<span class="hljs-number">1</span>];<br>        <span class="hljs-type">String</span> <span class="hljs-variable">up</span> <span class="hljs-operator">=</span> split[split.length - <span class="hljs-number">3</span>];<br>        <span class="hljs-type">String</span> <span class="hljs-variable">down</span> <span class="hljs-operator">=</span> split[split.length - <span class="hljs-number">2</span>];<br>        <span class="hljs-comment">//4 封装 outK outV</span><br>        outK.set(phone);<br>        outV.setUpFlow(Long.parseLong(up));<br>        outV.setDownFlow(Long.parseLong(down));<br>        outV.setSumFlow();<br>        <span class="hljs-comment">//5 写出 outK outV</span><br>        context.write(outK, outV);<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure>

<p>​	编写 Reducer 类  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.test.mapreduce.writable;<br><span class="hljs-keyword">import</span> org.apache.hadoop.io.Text;<br><span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.Reducer;<br><span class="hljs-keyword">import</span> java.io.IOException;<br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">FlowReducer</span> <span class="hljs-keyword">extends</span> <span class="hljs-title class_">Reducer</span>&lt;Text, FlowBean, Text, FlowBean&gt;<br>&#123;<br>    <span class="hljs-keyword">private</span> <span class="hljs-type">FlowBean</span> <span class="hljs-variable">outV</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">FlowBean</span>();<br>    <span class="hljs-meta">@Override</span><br>    <span class="hljs-keyword">protected</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">reduce</span><span class="hljs-params">(Text key, Iterable&lt;FlowBean&gt; values, Context</span><br><span class="hljs-params">    context)</span> <span class="hljs-keyword">throws</span> IOException, InterruptedException &#123;<br>        <span class="hljs-type">long</span> <span class="hljs-variable">totalUp</span> <span class="hljs-operator">=</span> <span class="hljs-number">0</span>;<br>        <span class="hljs-type">long</span> <span class="hljs-variable">totalDown</span> <span class="hljs-operator">=</span> <span class="hljs-number">0</span>;<br>        <span class="hljs-comment">//1 遍历 values,将其中的上行流量,下行流量分别累加</span><br>        <span class="hljs-keyword">for</span> (FlowBean flowBean : values) &#123;<br>            totalUp += flowBean.getUpFlow();<br>            totalDown += flowBean.getDownFlow();<br>        &#125;<br>        <span class="hljs-comment">//2 封装 outKV</span><br>        outV.setUpFlow(totalUp);<br>        outV.setDownFlow(totalDown);<br>        outV.setSumFlow();<br>        <span class="hljs-comment">//3 写出 outK outV</span><br>        context.write(key,outV);<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure>

<p>​	编写 Driver 驱动类  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.test.mapreduce.writable;<br><span class="hljs-keyword">import</span> org.apache.hadoop.conf.Configuration;<br><span class="hljs-keyword">import</span> org.apache.hadoop.fs.Path;<br><span class="hljs-keyword">import</span> org.apache.hadoop.io.Text;<br><span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.Job;<br><span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;<br><span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;<br><span class="hljs-keyword">import</span> java.io.IOException;<br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">FlowDriver</span> &#123;<br><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">main</span><span class="hljs-params">(String[] args)</span> <span class="hljs-keyword">throws</span> IOException, ClassNotFoundException, InterruptedException &#123;<br>        <span class="hljs-comment">//1 获取 job 对象</span><br>        <span class="hljs-type">Configuration</span> <span class="hljs-variable">conf</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Configuration</span>();<br>        <span class="hljs-type">Job</span> <span class="hljs-variable">job</span> <span class="hljs-operator">=</span> Job.getInstance(conf);<br>        <span class="hljs-comment">//2 关联本 Driver 类</span><br>        job.setJarByClass(FlowDriver.class);<br>        <span class="hljs-comment">//3 关联 Mapper 和 Reducer</span><br>        job.setMapperClass(FlowMapper.class);<br>        job.setReducerClass(FlowReducer.class);<br>        <span class="hljs-comment">//4 设置 Map 端输出 KV 类型</span><br>        job.setMapOutputKeyClass(Text.class);<br>        job.setMapOutputValueClass(FlowBean.class);<br>        <span class="hljs-comment">//5 设置程序最终输出的 KV 类型</span><br>        job.setOutputKeyClass(Text.class);<br>        job.setOutputValueClass(FlowBean.class);<br>        <span class="hljs-comment">//6 设置程序的输入输出路径</span><br>        FileInputFormat.setInputPaths(job, <span class="hljs-keyword">new</span> <span class="hljs-title class_">Path</span>(<span class="hljs-string">&quot;D:\\inputflow&quot;</span>));<br>        FileOutputFormat.setOutputPath(job, <span class="hljs-keyword">new</span> <span class="hljs-title class_">Path</span>(<span class="hljs-string">&quot;D:\\flowoutput&quot;</span>));<br>        <span class="hljs-comment">//7 提交 Job</span><br>        <span class="hljs-type">boolean</span> <span class="hljs-variable">b</span> <span class="hljs-operator">=</span> job.waitForCompletion(<span class="hljs-literal">true</span>);<br>        System.exit(b ? <span class="hljs-number">0</span> : <span class="hljs-number">1</span>);<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure>

<h2 id="MapReduce-框架原理"><a href="#MapReduce-框架原理" class="headerlink" title="MapReduce 框架原理"></a>MapReduce 框架原理</h2><h3 id="InputFormat-数据输入"><a href="#InputFormat-数据输入" class="headerlink" title="InputFormat 数据输入"></a>InputFormat 数据输入</h3><p>MapTask 的并行度决定 Map 阶段的任务处理并发度，进而影响到整个 Job 的处理速度</p>
<p>对于 1G 数据启动 8 个 MapTask 可以提高并发能力，但是对于 1 k 的数据启动 8 个是否合适呢？启动 8 MapTask 的时间远大于处理数据的时间。</p>
<h4 id="MapTask-并行度决定机制"><a href="#MapTask-并行度决定机制" class="headerlink" title="MapTask 并行度决定机制"></a>MapTask 并行度决定机制</h4><blockquote>
<p>数据块：Block 是 HDFS 物理上把数据分成一块一块。数据块是 HDFS 的存储数据单位</p>
<p>数据切片：数据切片是逻辑上对输入进行切片，并不是在磁盘上切片存储。数据切片是 MapReduce 计算输入数据的单位，一个切片会对应启动一个 MapTask。</p>
</blockquote>
<ol>
<li>一个 job 的 <strong>Map 阶段并行度</strong>由客户端在提交 job 时的<strong>切片数决定</strong></li>
<li>每一个 Split 切片分配一个 MapTask 并行实例处理</li>
<li>默认情况下，切片大小 &#x3D; BlockSize</li>
<li>切片时不考虑数据集整体（总收入的数据涉及每日收入存储文件），而是逐个针对每一个文件单独切片（当日交易数据）。</li>
</ol>
<h4 id="Job-提交流程源码解析"><a href="#Job-提交流程源码解析" class="headerlink" title="Job 提交流程源码解析"></a>Job 提交流程源码解析</h4><p><img src="/2022/07/17/Hadoop-MapReduce/image-20220715112714270.png" srcset="/img/loading.gif" lazyload alt="JobSubmit"></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs java">waitForCompletion()<br>submit();<br><span class="hljs-comment">// 1 建立连接</span><br>connect();<br><span class="hljs-comment">// 1）创建提交 Job 的代理</span><br><span class="hljs-keyword">new</span> <span class="hljs-title class_">Cluster</span>(getConfiguration());<br><span class="hljs-comment">// （1）判断是本地运行环境还是 yarn 集群运行环境</span><br>initialize(jobTrackAddr, conf);<br><span class="hljs-comment">// 2 提交 job</span><br>submitter.submitJobInternal(Job.<span class="hljs-built_in">this</span>, cluster)<br><span class="hljs-comment">// 1）创建给集群提交数据的 Stag 路径</span><br><span class="hljs-type">Path</span> <span class="hljs-variable">jobStagingArea</span> <span class="hljs-operator">=</span> JobSubmissionFiles.getStagingDir(cluster, conf);<br><span class="hljs-comment">// 2）获取 jobid ，并创建 Job 路径</span><br><span class="hljs-type">JobID</span> <span class="hljs-variable">jobId</span> <span class="hljs-operator">=</span> submitClient.getNewJobID();<br><span class="hljs-comment">// 3）拷贝 jar 包到集群</span><br>copyAndConfigureFiles(job, submitJobDir);<br>rUploader.uploadFiles(job, jobSubmitDir);<br><span class="hljs-comment">// 4）计算切片，生成切片规划文件</span><br>writeSplits(job, submitJobDir);<br>maps = writeNewSplits(job, jobSubmitDir);<br>input.getSplits(job);<br><span class="hljs-comment">// 5）向 Stag 路径写 XML 配置文件</span><br>writeConf(conf, submitJobFile);<br>conf.writeXml(out);<br><span class="hljs-comment">// 6）提交 Job,返回提交状态</span><br>status = submitClient.submitJob(jobId, submitJobDir.toString(),<br>job.getCredentials());<br></code></pre></td></tr></table></figure>

<h4 id="FileInputFormat-切片机制"><a href="#FileInputFormat-切片机制" class="headerlink" title="FileInputFormat 切片机制"></a>FileInputFormat 切片机制</h4><p><strong>切片机制</strong></p>
<blockquote>
<p>简单地按照文件的内容长度进行切片  </p>
<p>切片大小， 默认等于Block大小  </p>
<p>切片时不考虑数据集整体， 而是逐个针对每一个文件单独切片  </p>
</blockquote>
<p>例如</p>
<table>
<thead>
<tr>
<th>源文件</th>
<th>切片后</th>
</tr>
</thead>
<tbody><tr>
<td>file1.txt          320M</td>
<td>file1.txt.split1          0-128M</td>
</tr>
<tr>
<td></td>
<td>file1.txt.split2          128-256M</td>
</tr>
<tr>
<td></td>
<td>file1.txt.split3          256-320M</td>
</tr>
<tr>
<td>file2.txt          10m</td>
<td>file2.txt.split1          0-10M</td>
</tr>
</tbody></table>
<p><strong>源码解析</strong></p>
<ol>
<li><p>程序先找到你数据存储的目录。  </p>
</li>
<li><p>开始遍历处理（规划切片）目录下的每一个文件  </p>
</li>
<li><p>遍历第一个文件xx.txt</p>
<blockquote>
<ol>
<li>获取文件大小fs.sizeOf(xx.txt)  </li>
<li>计算切片大小   computeSplitSize(Math.max(minSize,Math.min(maxSize,blocksize)))&#x3D;blocksize&#x3D;128M  </li>
<li>默认情况下，切片大小&#x3D;blocksize  </li>
<li>开始切，形成第1个切片： xx.txt —0:128M 第2个切片 xx.txt —128:256M 第3个切片 xx.txt—256M:300M  （ 每次切片时，都要判断切完剩下的部分是否大于块的1.1倍，不大于1.1倍就划分一块切片）  </li>
<li>将切片信息写到一个切片规划文件中  </li>
<li>整个切片的核心过程在getSplit()方法中完成  </li>
<li>InputSplit只记录了切片的元数据信息，比如起始位置、长度以及所在的节点列表等</li>
</ol>
</blockquote>
</li>
<li><p>提交切片规划文件到YARN上， YARN上的MrAppMaster就可以根据切片规划文件计算开启MapTask个数</p>
</li>
</ol>
<p><strong>参数配置</strong> </p>
<blockquote>
<p><strong>源码中计算切片大小的公式</strong> </p>
<p>Math.max(minSize, Math.min(maxSize, blockSize));<br>mapreduce.input.fileinputformat.split.minsize&#x3D;1 默认值为1<br>mapreduce.input.fileinputformat.split.maxsize&#x3D; Long.MAXValue 默认值Long.MAXValue 因此， 默认情况下， 切片大小&#x3D;blocksize。  </p>
<p><strong>切片大小设置</strong>  </p>
<p>maxsize（ 切片最大值） ：参数如果调得比blockSize小， 则会让切片变小， 而且就等于配置的这个参数的值。<br>minsize（ 切片最小值） ：参数调的比blockSize大， 则可以让切片变得比blockSize还大。</p>
<p><strong>获取切片信息API</strong> </p>
<p>&#x2F;&#x2F; 获取切片的文件名称<br>String name &#x3D; inputSplit.getPath().getName();<br>&#x2F;&#x2F; 根据文件类型获取切片信息<br>FileSplit inputSplit &#x3D; (FileSplit) context.getInputSplit();  </p>
</blockquote>
<h4 id="TextInputFormat"><a href="#TextInputFormat" class="headerlink" title="TextInputFormat"></a>TextInputFormat</h4><p><strong>FileInputFormat 实现类</strong>  </p>
<p>在运行 MapReduce 程序时，输入的文件格式包括：基于行的日志文件、二进制格式文件、数据库表等。 针对不同的数据类型，FileInputFormat 提供了常见的接口实现类来解决包括： TextInputFormat、 KeyValueTextInputFormat、NLineInputFormat、 CombineTextInputFormat 和自定义 InputFormat 等。  </p>
<p><strong>TextInputFormat</strong>  </p>
<p>TextInputFormat 是默认的 FileInputFormat 实现类。按行读取每条记录。 键是存储该行在整个文件中的起始字节偏移量， LongWritable 类型。值是这行的内容，不包括任何行终止符（换行符和回车符）， Text 类型。  </p>
<table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td>源文件</td>
<td>Key&#x2F;Value</td>
</tr>
<tr>
<td>Rich learning form<br>Intelligent learning engine<br>Learning more convenient<br>From the real demand for more close to the enterprise</td>
<td>(0,Rich learning form)<br>(20,Intelligent learning engine)<br>(49,Learning more convenient)<br>(74,From the real demand for more close to the enterprise)</td>
</tr>
</tbody></table>
<h4 id="CombineTextInputFormat-切片机制"><a href="#CombineTextInputFormat-切片机制" class="headerlink" title="CombineTextInputFormat 切片机制"></a>CombineTextInputFormat 切片机制</h4><p>按文件规划切片，不管文件多小，都会单独一个切片（如果小文件很多，最后会合并产生好几个符合最大虚拟存储值的切片，MapTask 的数量是由切片数决定的），只交给一个 MapTask，用于解决大量小文件问题。</p>
<p><strong>虚拟存储切片最大值设置</strong></p>
<p>CombineTextInputFormat.setMaxInputSplitSize(job, 4194304);&#x2F;&#x2F; 4m<br>注意： 虚拟存储切片最大值设置最好根据实际的小文件大小情况来设置具体的值。  </p>
<p><strong>切片机制</strong></p>
<ol>
<li><p>虚拟存储过程</p>
<blockquote>
<ol>
<li>将输入目录下所有文件大小， 依次和设置的 setMaxInputSplitSize 值比较， 如果不大于设置的最大值， 逻辑上划分一个块。如果输入文件大于设置的最大值且大于两倍，那么以最大值切割一块； 当剩余数据大小超过设置的最大值且不大于最大值 2 倍，此时将文件均分成 2 个虚拟存储块（防止出现太小切片） 。<br>例如 setMaxInputSplitSize 值为 4M， 输入文件大小为 8.02M，则先逻辑上分成一个4M。 剩余的大小为 4.02M，如果按照 4M 逻辑划分，就会出现 0.02M 的小的虚拟存储文件， 所以将剩余的 4.02M 文件切分成（2.01M 和 2.01M）两个文件</li>
</ol>
</blockquote>
</li>
<li><p>切片过程</p>
<blockquote>
<ol>
<li><p>判断虚拟存储的文件大小是否大于 setMaxInputSplitSize 值，大于等于则单独<br>形成一个切片。  （主要是用于合并之后做判断，合并之后发现大于就不合了，小于就继续合）</p>
</li>
<li><p>如果不大于则跟下一个虚拟存储文件进行合并，共同形成一个切片。  </p>
</li>
<li><p>测试举例：有 4 个小文件大小分别为 1.7M、 5.1M、 3.4M 以及 6.8M 这四个小文件，则虚拟存储之后形成 6 个文件块，大小分别为：</p>
<p>1.7M，（2.55M、 2.55M） ， 3.4M 以及（3.4M、 3.4M）  </p>
<p>最终会形成 3 个切片，大小分别为：  </p>
<p>（1.7+2.55） M， （2.55+3.4） M， （3.4+3.4） M</p>
</li>
</ol>
</blockquote>
</li>
</ol>
<h3 id="MapReduce-工作流程"><a href="#MapReduce-工作流程" class="headerlink" title="MapReduce 工作流程"></a>MapReduce 工作流程</h3><p><img src="/2022/07/17/Hadoop-MapReduce/MR_Procedure.jpg" srcset="/img/loading.gif" lazyload alt="MapReduce Working Procedure"></p>
<h4 id="MapReduce-工作步骤"><a href="#MapReduce-工作步骤" class="headerlink" title="MapReduce 工作步骤"></a><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_44350553/article/details/109550564">MapReduce 工作步骤</a></h4><ol>
<li><p>首先有一个待处理的文本ss.txt 假设为200M大小</p>
</li>
<li><p>在客户端submit()之前，获取待处理的数据的信息，然后根据参数配置，形成一个任务分配的规划 。（默认128m一个数据块）</p>
<blockquote>
<p>ss.txt 0-128 任务1</p>
<p>ss.txt 128-200 任务2</p>
</blockquote>
</li>
<li><p>提交信息</p>
<blockquote>
<p>Job.split（任务切片信息）</p>
<p>wc.jar（需要提交的jar包）</p>
<p>Job.xml（xml配置文件）</p>
<p>将这三个文件从MapReduce客户端提交到Yarn上的ResourceManager上进行处理。</p>
</blockquote>
</li>
<li><p>4.Yarn上提交时，会将每个任务封装成一个job，提交给yarn处理，ResourceManager会计算出MapTask数量（和切片数量一致）然后RM把任务分配给NodeMamager，在MR appmaster允许后，NodeManager就会来处理相应的任务（Maptask1&amp;Maptask2)，每个任务会并行执行。</p>
</li>
<li><p>MapTask会执行Mapper中的map方法，此方法需要传入k，v值，所以我们需要先从数据中获取k，v值，以作为输入的参数具体做法是：首先调用InputFormat方法，默认为TextInputFormat方法，在此方法中调用createRecordReader方法，将每个块封装（k,v）键值对，然后传递给map方法。</p>
</li>
<li><p>数据进入MapTask中以后会进行Map端的逻辑运算，运算完后，会进行写操作。</p>
</li>
<li><p>map端产生的数据如果直接进行写操作，写入到reduce中，会直接操作磁盘，这样就会进行大量的io操作，效率太低，所以map端reduce端之间会进行一个shuffle操作。</p>
</li>
<li><p>所以map端产生数据后会通过outputCollector向环形缓冲区写入数据，环形缓冲区分为两部分，一部分写入文件的元数据信息，另一部分写入文件的真实内容。环形缓冲区默认大小为100M，环形缓冲区写入80%数据以后，会反向溢写。</p>
</li>
<li><p>在溢写之前会对环形缓冲区中数据会按照指定的分区排序规则进行分区和排序，之所以反向溢写是为了可以边接收数据边向磁盘中溢写数据。</p>
</li>
<li><p>在分区和排序过后会把文件溢写到磁盘当中，可能发生多次溢写，可能溢写到多个文件。</p>
</li>
<li><p>对所有溢写到磁盘中的文件进行Merge归并排序操作。</p>
</li>
<li><p>在溢写到磁盘后和对磁盘中文件归并排序之前可能进行combine合并操作，它的意义是对每个MapTask输出的数据进行局部汇总，以减少网络传输量。</p>
<blockquote>
<p>第一：在Map阶段，由于map的进程数量是多于reduce的，所以map阶段处理的效率更高<br>第二：在Map阶段进行合并，这样传递给reduce的数据<br>会少很多。<br>第三：combine操作能够应用的前提是不能够影响最终的业务逻辑，combine的输出的kv要和reduce输入的kv对应起来。</p>
</blockquote>
</li>
</ol>
<p><strong>Map 阶段汇总</strong></p>
<p>宏观上看，MapTask阶段分为 Read阶段，Map阶段，Collect阶段，和溢写（spill）阶段, Merge 阶段  </p>
<blockquote>
<p>Read阶段:MapTask运用用户编写的RecordReader方法，从输入的inputsplit中解析出key&#x2F;value</p>
<p>Map阶段：将key&#x2F;value值放入用户编写的map（）方法中，产生一系列新的key&#x2F;value值。</p>
<p>Collect阶段，将用户在map（）阶段处理完成的每个key&#x2F;value数据，调用OutputCollector.collect（）方法，输出结果。在函数内部，它将会生成key&#x2F;value的分区（调用partioner），并写入到一个环形缓冲区当中。</p>
<p>spill阶段，即溢写，当环形缓冲区数据满时，MapReduce会将数据写入到本地磁盘上，此时会生成一个临时文件，需要注意的是，在数据写入到磁盘前，会对数据进行一次本地排序，有必要时，还会对数据进行合并，压缩等操作。</p>
<p>Merge 阶段：当所有数据处理完成后， MapTask 对所有临时文件进行一次合并，以确保最终只会生成一个数据文件。</p>
<p>当所有数据处理完后， MapTask 会将所有临时文件合并成一个大文件， 并保存到文件output&#x2F;file.out 中，同时生成相应的索引文件 output&#x2F;file.out.index。<br>在进行文件合并过程中， MapTask 以分区为单位进行合并。对于某个分区， 它将采用多轮递归合并的方式。 每轮合并 mapreduce.task.io.sort.factor（默认 10） 个文件，并将产生的文件重新加入待合并列表中，对文件排序后，重复以上过程，直到最终得到一个大文件。让每个 MapTask 最终只生成一个数据文件，可避免同时打开大量文件和同时读取大量小文件产生的随机读取带来的开销。  </p>
<p>MRAppmaster职能：启动MapTask任务</p>
</blockquote>
<ol start="12">
<li>所有MapTask数据处理完成后，启动相印数量的ReduceTask，并告知ReduceTask需要处理数据的范围（数据分区）</li>
<li>ReduceTask将MapTask中的数据下载到ReduceTask本地磁盘，然后合并不同的文件，进行归并排序。</li>
<li>最后将数据交给Reduce处理，一次读区一组数据。</li>
<li>最后通过OutputFormat的RecordWriter方法将数据写入到本地磁盘的文件当中。</li>
</ol>
<p><strong>Reduce 阶段汇总</strong></p>
<p>1.Copy 2.Merge 3.Sort 4.Reduce</p>
<blockquote>
<p>copy: ReduceTask将远程从MapTask上复制过来要处理的数据，针对某一片数据，如果数据的大小超过一个阈值，则直接存储在磁</p>
<p>盘中，否则直接放到内存中。</p>
<p>Merge：ReduceTask在远程复制的同时，后台启动了两个线程，将硬盘和内存中的数据进行合并，样可以避免内存占用过多，或者磁盘文件过多。</p>
<p>sort:按照MapReduce的语义，Reduce()的输入值是按照key进行聚集的一组数据，为了将key相同的数据放在一起，hadoop采用了基于排序的策略，由于MapTask阶段已经进行了局部排序，所以ReduceTask阶段只需要对所有数据进行一次归并排序即可</p>
<p>Reduce：Reducer会调用reduce()方法将处理好的数据写入到HDFS当中。</p>
</blockquote>
<p><strong>设置 ReduceTask 并行度 (个数)</strong></p>
<p> ReduceTask 的并行度同样影响整个 Job 的执行并发度和执行效率，但与 MapTask 的并发数由切片数决定不同， ReduceTask 数量的决定是可以直接手动设置：  </p>
<blockquote>
<p>&#x2F;&#x2F; 默认值是 1，手动设置为 4<br>job.setNumReduceTasks(4);  </p>
</blockquote>
<p>注:</p>
<ol>
<li>ReduceTask&#x3D;0， 表示没有Reduce阶段， 输出文件个数和Map个数一致。</li>
<li>ReduceTask默认值就是1， 所以输出文件个数为一个。</li>
<li>如果数据分布不均匀， 就有可能在Reduce阶段产生数据倾斜</li>
<li>ReduceTask数量并不是任意设置， 还要考虑业务逻辑需求， 有些情况下， 需要计算全局汇总结果， 就只能有1个ReduceTask。</li>
<li>具体多少个ReduceTask， 需要根据集群性能而定。</li>
<li>如果分区数不是1， 但是ReduceTask为1， 是否执行分区过程。 答案是：不执行分区过程。 因为在MapTask的源码中， 执行分区的前提是先判断ReduceNum个数是否大于1。 不大于1肯定不执行</li>
</ol>
<h4 id="shuffle"><a href="#shuffle" class="headerlink" title="shuffle"></a>shuffle</h4><p><strong>工作流程</strong></p>
<p><img src="/2022/07/17/Hadoop-MapReduce/image-20220716154211817.png" srcset="/img/loading.gif" lazyload alt="Shuffle Working Procedure"></p>
<p>Shuffle是指在map()方法之后，reduce()方法之前所进行的数据处理过程，shuffle的流程详解如下：</p>
<ol>
<li>首先MapTask的Map方法将输出的(k，v)数据放入到环形缓冲区中。</li>
<li>当环形缓冲区数据达到80%时，会将数据不断溢写到磁盘当中，有可能溢出多个文件，多个溢出的文件会合并成为一个大的文件。</li>
<li>在溢出的过程以及合并的过程中，会调用partioner对磁盘中的数据会进行分区，进行按照key排序。</li>
<li>reduceTask会根据自身的分区号结合map端数据，取出相应的MapTask中的分区数据。</li>
<li>ReduceTask会储存来自不同MapTask的结果文件进行归并，排序。</li>
<li>当合并成大文件之后，shuffle过程就结束了，此时会进入reduce()方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://www.cnblogs.com/yin1361866686/p/11732071.html">spill</a></strong></p>
<p>spill线程为这次spill过程创建一个磁盘文件（从所有本地目录中轮询查找拥有足够大空间的目录，找到之后在该目录下创建一个类似“spillxx.out”的文件）。spill线程根据排过序的kvmeta逐个将partition的数据写入该文件，直到所有的partition都写完。一个partition在文件中对应的数据叫“段（segment）”。</p>
<p>partition在文件中的索引信息是由一个三元组记录的，该三元组包括：起始位置、原始数据长度、压缩之后的数据长度，一个partition对应一个三元组。所有的索引信息是存放在内存中的，若果内存空间不足了就会把后续的索引信息写到磁盘中。</p>
<p>从所有的本地目录总轮询查找拥有足够大空间的目录，在该目录下创建一个类似“spillxx.out.index”的文件，文件中不光存储了索引数据还存储了crc32的校验数据（spillxx.out.index文件和spillxx.out文件不一定是在同一个目录下）。</p>
<p>每一次spill过程会生成至少一个out文件，有时还会生成index文件，spill的次数也会在文件名中显现出来。</p>
<p><img src="/2022/07/17/Hadoop-MapReduce/image-20220716154914368.png" srcset="/img/loading.gif" lazyload alt="spill"></p>
<p>溢写阶段详情：<br>步骤 1： 利用快速排序算法对缓存区内的数据进行排序，排序方式是，<strong>先按照分区编号Partition 进行排序</strong>，<strong>然后按照 key 进行排序</strong>。这样， 经过排序后，数据以分区为单位聚集在一起，且同一分区内所有数据按照 key 有序。</p>
<p>步骤 2： 按照分区编号由小到大依次将每个分区中的数据写入任务工作目录下的临时文件 output&#x2F;spillN.out（N 表示当前溢写次数）中。如果用户设置了 Combiner，则写入文件之前，对每个分区中的数据进行一次聚集操作。</p>
<p>步骤 3： 将分区数据的元信息写到内存索引数据结构 SpillRecord 中，其中每个分区的元信息包括在临时文件中的偏移量、压缩前数据大小和压缩后数据大小。如果当前内存索引大小超过 1MB，则将内存索引写到文件 output&#x2F;spillN.out.index 中。  </p>
<h3 id="MapReduce-优化"><a href="#MapReduce-优化" class="headerlink" title="MapReduce 优化"></a>MapReduce 优化</h3><p><strong>Map 阶段</strong></p>
<ol>
<li><p><strong>减少溢写的次数</strong></p>
<blockquote>
<p>mapreduce.task.io.sort.mb</p>
<p>Shuffle的环形缓冲区大小，默认100m，可以提高到200m</p>
<p>mapreduce.map.sort.spill.percent</p>
<p>环形缓冲区溢出的阈值，默认80% ，可以提高的90%</p>
</blockquote>
</li>
<li><p><strong>增加每次Merge合并次数</strong></p>
<blockquote>
<p>mapreduce.task.io.sort.factor默认10，可以提高到20</p>
</blockquote>
</li>
<li><p><strong>在不影响业务结果的前提条件下可以提前采用Combiner</strong></p>
<blockquote>
<p>job.setCombinerClass(xxxReducer.class);</p>
</blockquote>
</li>
<li><p><strong>异常重试</strong></p>
<blockquote>
<p>mapreduce.map.maxattempts每个Map Task最大重试次数，一旦重试次数超过该值，则认为Map Task运行失败，默认值：4。根据机器性能适当提高。</p>
</blockquote>
</li>
</ol>
<p><strong>Reduce阶段</strong></p>
<ol>
<li><p><strong>合理设置Map和Reduce数</strong></p>
<blockquote>
<p>两个都不能设置太少，也不能设置太多。太少，会导致Task等待，延长处理时间；太多，会导致 Map、Reduce任务间竞争资源，造成处理超时等错误。</p>
</blockquote>
</li>
<li><p><strong>设置Map、Reduce共存</strong></p>
<blockquote>
<p>调整mapreduce.job.reduce.slowstart.completedmaps当MapTask完成的比例达到该值后才会为ReduceTask申请资源。默认是0.05。</p>
</blockquote>
</li>
<li><p><strong>规避使用Reduce，因为Reduce在用于连接数据集的时候将会产生大量的网络消耗。</strong></p>
</li>
<li><p><strong>增加每个Reduce去Map中拿数据的并行数</strong></p>
<blockquote>
<p>mapreduce.reduce.shuffle.parallelcopies每个Reduce去Map中拉取数据的并行数，默认值是5。可以提高到10。</p>
</blockquote>
</li>
<li><p><strong>集群性能可以的前提下，增大Reduce端存储数据内存的大小</strong>。</p>
<blockquote>
<p>mapreduce.reduce.memory.mb 默认ReduceTask内存上限1024MB，根据128m数据对应1G内存原则，适当提高内存到4-6G</p>
</blockquote>
</li>
<li><p><strong>异常重试</strong></p>
<blockquote>
<p>mapreduce.reduce.maxattempts每个Reduce Task最大重试次数，一旦重试次数超过该值，则认为Reduce Task运行失败，默认值：4。</p>
</blockquote>
</li>
<li><p><strong>mapreduce.reduce.shuffle.input.buffer.percent</strong></p>
<blockquote>
<p>Buffer大小占Reduce可用内存的比例，默认值0.7。可以提高到0.8</p>
</blockquote>
</li>
<li><p>mapreduce.reduce.shuffle.merge.percent </p>
<blockquote>
<p>Buffer中的数据达到多少比例开始写入磁盘，默认值0.66。可以提高到0.75</p>
</blockquote>
</li>
</ol>
<p><strong>IO传输</strong></p>
<p>采用数据压缩的方式，减少网络IO的的时间。安装Snappy和LZOP压缩编码器。</p>
<p><strong>为了减少磁盘和传输IO，可以采用Snappy或者LZO压缩</strong></p>
<blockquote>
<p>conf.setBoolean(“mapreduce.map.output.compress”, true);</p>
<p>conf.setClass(“mapreduce.map.output.compress.codec”, SnappyCodec.class,CompressionCodec.class);</p>
<ol>
<li>map输入端主要考虑数据量大小和切片，支持切片的有Bzip2、LZO。注意：LZO要想支持切片必须创建索引；</li>
<li>map输出端主要考虑速度，速度快的snappy、LZO；</li>
<li>reduce输出端主要看具体需求，例如作为下一个mr输入需要考虑切片，永久保存考虑压缩率比较大的gzip。</li>
</ol>
</blockquote>
<p><strong>整体</strong></p>
<ol>
<li><p><strong>自定义分区，减少数据倾斜</strong></p>
<blockquote>
<p>定义类，继承Partitioner接口，重写getPartition方法</p>
</blockquote>
</li>
<li><p><strong>NodeManager默认内存8G，需要根据服务器实际配置灵活调整</strong></p>
<blockquote>
<p>例如128G内存，配置为100G内存左右，yarn.nodemanager.resource.memory-mb</p>
</blockquote>
</li>
<li><p><strong>单容器默认内存8G，需要根据该任务的数据量灵活调整</strong></p>
<blockquote>
<p>例如128m数据，配置1G内存，yarn.scheduler.maximum-allocation-mb。</p>
</blockquote>
</li>
<li><p><strong>mapreduce.map.memory.mb</strong></p>
<blockquote>
<p>控制分配给MapTask内存上限，如果超过会kill掉进程（报：Container is running beyond physical memory limits. Current usage:565MB of512MB physical memory used；Killing Container）。默认内存大小为1G，如果数据量是128m，正常不需要调整内存；如果数据量大于128m，可以增加MapTask内存，最大可以增加到4-5g。</p>
</blockquote>
</li>
<li><p><strong>mapreduce.reduce.memory.mb</strong></p>
<blockquote>
<p>控制分配给ReduceTask内存上限。默认内存大小为1G，如果数据量是128m，正常不需要调整内存；如果数据量大于128m，可以增加ReduceTask内存大小为4-5g。</p>
</blockquote>
</li>
<li><p><strong>mapreduce.map.java.opts</strong></p>
<blockquote>
<p>控制MapTask堆内存大小。（如果内存不够，报：java.lang.OutOfMemoryError）</p>
</blockquote>
</li>
<li><p><strong>mapreduce.reduce.java.opts</strong></p>
<blockquote>
<p>控制ReduceTask堆内存大小。（如果内存不够，报：java.lang.OutOfMemoryError）</p>
</blockquote>
</li>
<li><p><strong>mapreduce.task.timeout</strong></p>
<blockquote>
<p>如果一个Task在一定时间内没有任何进入，即不会读取新的数据，也没有输出数据，则认为该Task处于Block状态，可能是卡住了，也许永远会卡住，为了防止因为用户程序永远Block住不退出，则强制设置了一个该超时时间（单位毫秒），默认是600000（10分钟）。如果你的程序对每条输入数据的处理时间过长，建议将该参数调大。</p>
</blockquote>
</li>
<li><p><strong>可以增加MapTask的CPU核数，增加ReduceTask的CPU核数</strong></p>
</li>
<li><p><strong>增加每个Container的CPU核数和内存大小</strong></p>
</li>
<li><p><strong>在hdfs-site.xml文件中配置多目录（多磁盘）</strong></p>
</li>
</ol>
<h3 id="Partition-分区"><a href="#Partition-分区" class="headerlink" title="Partition 分区"></a>Partition 分区</h3><p>要求将统计结果按照条件输出到不同文件中（ 分区） 。  </p>
<p><strong>默认Partitioner分区</strong> </p>
<p>默认分区是根据key的hashCode对ReduceTasks个数取模得到的。用户没法控制哪个key存储到哪个分区。  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">HashPartitioner</span>&lt;K, V&gt; <span class="hljs-keyword">extends</span> <span class="hljs-title class_">Partitioner</span>&lt;K, V&gt; &#123;<br>    <span class="hljs-keyword">public</span> <span class="hljs-type">int</span> <span class="hljs-title function_">getPartition</span><span class="hljs-params">(K key, V value, <span class="hljs-type">int</span> numReduceTasks)</span> &#123;<br>        <span class="hljs-keyword">return</span> (key.hashCode() &amp; Integer.MAX_VALUE) % numReduceTasks;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure>

<p><strong>自定义Partitioner步骤</strong></p>
<ol>
<li><p>自定义类继承Partitioner，重写getPartition()方法  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">CustomPartitioner</span> <span class="hljs-keyword">extends</span> <span class="hljs-title class_">Partitioner</span>&lt;Text, FlowBean&gt; &#123;<br>    <span class="hljs-meta">@Override</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-type">int</span> <span class="hljs-title function_">getPartition</span><span class="hljs-params">(Text key, FlowBean value, <span class="hljs-type">int</span> numPartitions)</span> &#123;<br>        <span class="hljs-comment">// 控制分区代码逻辑</span><br>        … …<br>        <span class="hljs-keyword">return</span> partition;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure>
</li>
<li><p>在Job驱动中， 设置自定义Partitioner  </p>
<blockquote>
<p>job.setPartitionerClass(CustomPartitioner.class);  </p>
</blockquote>
</li>
<li><p>自定义Partition后，要根据自定义Partitioner的逻辑设置相应数量的ReduceTask  </p>
<blockquote>
<p>job.setNumReduceTasks(5);</p>
</blockquote>
</li>
</ol>
<p><strong>分区总结</strong></p>
<ol>
<li><p>如果ReduceTask的数量&gt; getPartition的结果数， 则会多产生几个空的输出文件part-r-000xx</p>
<blockquote>
<p>job.setNumReduceTasks(6);  大于5， 程序会正常运行， 会产生空文件  </p>
</blockquote>
</li>
<li><p>如果1&lt;ReduceTask的数量&lt;getPartition的结果数， 则有一部分分区数据无处安放， 会Exception</p>
</li>
<li><p>如果ReduceTask的数量&#x3D;1， 则不管MapTask端输出多少个分区文件， 最终结果都交给这一个ReduceTask， 最终也就只会产生一个结果文件 part-r-00000；  </p>
<blockquote>
<p>job.setNumReduceTasks(1);  	会正常运行，只不过会产生一个输出文件  </p>
</blockquote>
</li>
<li><p>分区号必须从零开始， 逐一累加。</p>
</li>
</ol>
<p><strong>案例实操</strong></p>
<ol>
<li><p>增加自定义分区类</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.test.mapreduce.partitionercompable;<br><span class="hljs-keyword">import</span> org.apache.hadoop.io.Text;<br><span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.Partitioner;<br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">ProvincePartitioner2</span> <span class="hljs-keyword">extends</span> <span class="hljs-title class_">Partitioner</span>&lt;FlowBean, Text&gt; &#123;<br>    <span class="hljs-meta">@Override</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-type">int</span> <span class="hljs-title function_">getPartition</span><span class="hljs-params">(FlowBean flowBean, Text text, <span class="hljs-type">int</span> numPartitions)</span><br>    &#123;<br>        <span class="hljs-comment">//获取手机号前三位</span><br>        <span class="hljs-type">String</span> <span class="hljs-variable">phone</span> <span class="hljs-operator">=</span> text.toString();<br>        <span class="hljs-type">String</span> <span class="hljs-variable">prePhone</span> <span class="hljs-operator">=</span> phone.substring(<span class="hljs-number">0</span>, <span class="hljs-number">3</span>);<br>        <span class="hljs-comment">//定义一个分区号变量 partition,根据 prePhone 设置分区号</span><br>        <span class="hljs-type">int</span> partition;<br>        <span class="hljs-keyword">if</span>(<span class="hljs-string">&quot;136&quot;</span>.equals(prePhone))&#123;<br>        	partition = <span class="hljs-number">0</span>;<br>        &#125;<span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span>(<span class="hljs-string">&quot;137&quot;</span>.equals(prePhone))&#123;<br>        	partition = <span class="hljs-number">1</span>;<br>        &#125;<span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span>(<span class="hljs-string">&quot;138&quot;</span>.equals(prePhone))&#123;<br>        	partition = <span class="hljs-number">2</span>;<br>        &#125;<span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span>(<span class="hljs-string">&quot;139&quot;</span>.equals(prePhone))&#123;<br>        	partition = <span class="hljs-number">3</span>;<br>        &#125;<span class="hljs-keyword">else</span> &#123;<br>        	partition = <span class="hljs-number">4</span>;<br>        &#125;<br>        <span class="hljs-comment">//最后返回分区号 partition</span><br>        <span class="hljs-keyword">return</span> partition;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure>
</li>
<li><p>在驱动类中添加分区类  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-comment">// 设置自定义分区器</span><br>job.setPartitionerClass(ProvincePartitioner2.class);<br><span class="hljs-comment">// 设置对应的 ReduceTask 的个数</span><br>job.setNumReduceTasks(<span class="hljs-number">5</span>);<br></code></pre></td></tr></table></figure></li>
</ol>
<h3 id="Combiner-合并"><a href="#Combiner-合并" class="headerlink" title="Combiner 合并"></a>Combiner 合并</h3><ol>
<li><p>Combiner是MR程序中Mapper和Reducer之外的一种组件。</p>
</li>
<li><p>Combiner组件的父类就是Reducer。</p>
</li>
<li><p>Combiner和Reducer的区别在于运行的位置</p>
<blockquote>
<p>Combiner是在每一个MapTask所在的节点运行;</p>
<p>Reducer是接收全局所有Mapper的输出结果；  </p>
</blockquote>
</li>
<li><p>Combiner的意义就是对每一个MapTask的输出进行局部汇总， 以减小网络传输量。</p>
</li>
<li><p>Combiner能够应用的前提是不能影响最终的业务逻辑， 而且， Combiner的输出kv<br>应该跟Reducer的输入kv类型要对应起来。</p>
</li>
</ol>
<p><strong>自定义 Combiner 实现步骤</strong>  </p>
<ol>
<li><p>自定义一个 Combiner 继承 Reducer，重写 Reduce 方法 </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">WordCountCombiner</span> <span class="hljs-keyword">extends</span> <span class="hljs-title class_">Reducer</span>&lt;Text, IntWritable, Text,<br>IntWritable&gt; &#123;<br>    <span class="hljs-keyword">private</span> <span class="hljs-type">IntWritable</span> <span class="hljs-variable">outV</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">IntWritable</span>();<br>    <span class="hljs-meta">@Override</span><br>    <span class="hljs-keyword">protected</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">reduce</span><span class="hljs-params">(Text key, Iterable&lt;IntWritable&gt; values, Context</span><br><span class="hljs-params">    context)</span> <span class="hljs-keyword">throws</span> IOException, InterruptedException &#123;<br>        <span class="hljs-type">int</span> <span class="hljs-variable">sum</span> <span class="hljs-operator">=</span> <span class="hljs-number">0</span>;<br>        <span class="hljs-keyword">for</span> (IntWritable value : values) &#123;<br>            sum += value.get();<br>        &#125;<br>        outV.set(sum);<br>        context.write(key,outV);<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure>
</li>
<li><p>在 Job 驱动类中设置：  </p>
<blockquote>
<p>job.setCombinerClass(WordCountCombiner.class);</p>
</blockquote>
</li>
</ol>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/BigData/" class="category-chain-item">BigData</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/Hadoop/">#Hadoop</a>
      
        <a href="/tags/MapReduce/">#MapReduce</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>MapReduce</div>
      <div>http://example.com/2022/07/17/Hadoop-MapReduce/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>Zhao Zhuoyue</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2022年7月17日</div>
        </div>
      
      
      <div class="license-meta-item">
        <div>许可协议</div>
        <div>
          
            
            
              <a target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
              <span class="hint--top hint--rounded" aria-label="BY - 署名">
                <i class="iconfont icon-by"></i>
              </span>
              </a>
            
          
        </div>
      </div>
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2022/07/17/Hadoop-Yarn/" title="MapReduce">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">MapReduce</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2022/07/17/Hive/" title="MapReduce">
                        <span class="hidden-mobile">MapReduce</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      headingSelector : CONFIG.toc.headingSelector || 'h1,h2,h3,h4,h5,h6',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      collapseDepth   : CONFIG.toc.collapseDepth || 0,
      scrollSmooth    : true,
      headingsOffset  : -boardTop
    });
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.10/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
